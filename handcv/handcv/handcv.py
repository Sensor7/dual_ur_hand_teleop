"""
Find the 3D pose of a human hand using computer vision.

Using cv_bridge, take messages from the /image_raw topic and convert them
into OpenCV images for mediapipe to use to figure out where a user's hand
is located. Afterwards, figure out the 3D locations of the hand's joints
generated by mediapipe using the depth_image_proc package.

SUBSCRIBERS:
  + /camera/color/image_raw (Image) - Raw image data from the RealSense D435i
  camera.
  + /camera/aligned_depth_to_color/image_raw (Image) - Aligned depth image from
  the RealSense D435i camera.
PUBLISHERS:
  + /cv_image (Image) - Annotated image with the 3D location of the hand's
  pose.
  + /waypoint (PoseStamped) - The 3D location of the hand's pose.
  + /right_gesture (String) - The gesture that the right hand is making.

"""
import rclpy
from rclpy.node import Node
from rclpy.callback_groups import MutuallyExclusiveCallbackGroup

from sensor_msgs.msg import Image
from visualization_msgs.msg import Marker
from geometry_msgs.msg import PoseStamped
from std_msgs.msg import String
from hand_interfaces.msg import FingerData

from cv_bridge import CvBridge, CvBridgeError
from .mediapipehelper import MediaPipeRos as mps

import PIL.Image as PILImage
import mediapipe as mp
import numpy as np
import cv2 as cv
import torch
from transformers import AutoImageProcessor, AutoModelForDepthEstimation
from scipy.spatial.transform import Rotation as R


# Initialize the Depth Anything model
model_name = "Intel/dpt-hybrid-midas"
processor = AutoImageProcessor.from_pretrained(model_name)
model = AutoModelForDepthEstimation.from_pretrained(model_name)
# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class HandCV(Node):
    def __init__(self):
        super().__init__("handcv")

        # initialize CvBridge object
        self.bridge = CvBridge()

        # initialize MediaPipe Object
        self.mps = mps()

        # create callback groups
        self.timer_callback_group = MutuallyExclusiveCallbackGroup()

        # create timer
        self.timer = self.create_timer(
            1/30, self.timer_callback, callback_group=self.timer_callback_group)

        # create subscribers
        self.color_image_raw_sub = self.create_subscription(
            Image, '/image_raw', self.color_image_raw_callback, 10)

        # create publishers
        self.cv_image_pub = self.create_publisher(Image, 'cv_image', 10)

        self.right_waypoint_pub = self.create_publisher(
                PoseStamped, 'right_waypoint', 10)

        self.left_waypoint_pub = self.create_publisher(
                PoseStamped, 'left_waypoint', 10)

        self.right_gesture_pub = self.create_publisher(
                String, 'right_gesture', 10)
        
        self.left_gesture_pub = self.create_publisher(
                String, 'left_gesture', 10)

        # intialize other variables
        self.color_image = None
        self.depth_image = None
        self.left_waypoint = PoseStamped() 
        self.left_waypoint.pose.orientation.x = 1.0
        self.left_waypoint.pose.orientation.w = 0.0
        self.right_waypoint = PoseStamped() 
        self.right_waypoint.pose.orientation.x = 1.0
        self.right_waypoint.pose.orientation.w = 0.0
        self.image_width = 0
        self.image_height = 0
        self.left_centroid = np.array([0.0, 0.0, 0.0])
        self.right_centroid = np.array([0.0, 0.0, 0.0])
        self.left_rotation = 0.0
        self.right_rotation = 0.0
        self.use_gpu = False

    def depth_image_from_color_image(self, color_image):
        inputs = processor(images=color_image, return_tensors="pt").to(device)

        # Perform inference
        with torch.no_grad():
            outputs = model(**inputs)
            predicted_depth = outputs.predicted_depth
        
        depth_map = torch.nn.functional.interpolate(
            predicted_depth.unsqueeze(1),
            size=color_image.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze().cpu().numpy()
        depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
        depth_map = (depth_map * 255).astype(np.uint8)
        return depth_map


    def color_image_raw_callback(self, msg):
        """Cpature color images and convert them to OpenCV images."""
        self.color_image = self.bridge.imgmsg_to_cv2(
            msg, desired_encoding="rgb8")
        self.color_image = cv.flip(self.color_image, 1)
        color_image = self.color_image.copy()
        if PILImage.fromarray(color_image).mode != 'RGB':
            color_image = np.array(PILImage.fromarray(color_image).convert('RGB'))
        if self.use_gpu:
            self.depth_image = self.depth_image_from_color_image(color_image)
        else:
            self.depth_image = np.zeros_like(self.color_image)
        self.image_width = msg.width
        self.image_height = msg.height

    def process_depth_image(self, annotated_image=None, detection_result=None):
        """
        Process the depth image to find the 3D location of the hand's pose.

        Args:
        ----
        annotated_image (np.array): The annotated image with the 3D location of
        the hand's pose.
        detection_result (mediapipe): The result of the hand landmark detection.

        Returns:
        -------
        cv_image (Image): The annotated image with the 3D location of the hand's
        pose.
        right_gesture (String): The gesture that the right hand is making.

        """
        right_gesture = "None"
        right_index = None
        left_gesture = "None"
        left_index = None
        if detection_result.gestures and detection_result.handedness:
            if len(detection_result.handedness) == 2:
                if detection_result.handedness[0][0].category_name == "Left":
                    left_gesture = detection_result.gestures[0][0].category_name
                    right_gesture = detection_result.gestures[1][0].category_name
                    left_index = 0
                    right_index = 1
                elif detection_result.handedness[0][0].category_name == "Right":
                    right_gesture = detection_result.gestures[0][0].category_name
                    left_gesture = detection_result.gestures[1][0].category_name
                    left_index = 1
                    right_index = 0
            elif len(detection_result.handedness) == 1:
                if detection_result.handedness[0][0].category_name == "Left":
                    left_gesture = detection_result.gestures[0][0].category_name
                    right_gesture = "None"
                    left_index = 0
                elif detection_result.handedness[0][0].category_name == "Right":
                    right_gesture = detection_result.gestures[0][0].category_name
                    left_gesture = "None"
                    right_index = 0

        if detection_result.hand_landmarks and right_index is not None:
            # self.get_logger().info("Right Hand")
            right_coords = np.array([[landmark.x * np.shape(annotated_image)[1],
                                landmark.y * np.shape(annotated_image)[0]]
                               for landmark in [detection_result.hand_landmarks[right_index][0],
                                                detection_result.hand_landmarks[right_index][1],
                                                detection_result.hand_landmarks[right_index][2],
                                                detection_result.hand_landmarks[right_index][5],
                                                detection_result.hand_landmarks[right_index][9],
                                                detection_result.hand_landmarks[right_index][14],
                                                detection_result.hand_landmarks[right_index][17]]])
        # now perform the math on the numpy arrays. I think this is faster?
            length = right_coords.shape[0]
            sum_x = np.sum(right_coords[:, 0])
            sum_y = np.sum(right_coords[:, 1])
            self.right_centroid = np.array([sum_x/length, sum_y/length, 0.0])

            rotation = []
            hand_landmark = detection_result.hand_landmarks[right_index]
            self.right_rotation = self.mps.calculate_hand_rotation(hand_landmark)

            if self.use_gpu:
                self.right_centroid[2] = self.depth_image[int(self.right_centroid[1]), int(self.right_centroid[0])]
            else:
                self.right_centroid[2] = 40.0

        if detection_result.hand_landmarks and left_index is not None:
            left_coords = np.array([[landmark.x * np.shape(annotated_image)[1],
                                landmark.y * np.shape(annotated_image)[0]]
                               for landmark in [detection_result.hand_landmarks[left_index][0],
                                                detection_result.hand_landmarks[left_index][1],
                                                detection_result.hand_landmarks[left_index][2],
                                                detection_result.hand_landmarks[left_index][5],
                                                detection_result.hand_landmarks[left_index][9],
                                                detection_result.hand_landmarks[left_index][14],
                                                detection_result.hand_landmarks[left_index][17]]])
            length = left_coords.shape[0]
            sum_x = np.sum(left_coords[:, 0])
            sum_y = np.sum(left_coords[:, 1])
            self.left_centroid = np.array([sum_x/length, sum_y/length, 0.0])

            rotation = []
            hand_landmark = detection_result.hand_landmarks[left_index]
            self.left_rotation = self.mps.calculate_hand_rotation(hand_landmark)


            if self.use_gpu:
                self.left_centroid[2] = self.depth_image[int(self.left_centroid[1]), int(self.left_centroid[0])]
            else:
                self.left_centroid[2] = 0.5

        self.right_waypoint.pose.position.x = self.right_centroid[0]
        self.right_waypoint.pose.position.y = self.right_centroid[1]
        self.right_waypoint.pose.position.z = self.right_centroid[2]
        r = R.from_euler('x', self.right_rotation, degrees=True)
        quat = r.as_quat()
        self.right_waypoint.pose.orientation.x = quat[0]
        self.right_waypoint.pose.orientation.y = quat[1]
        self.right_waypoint.pose.orientation.z = quat[2]
        self.right_waypoint.pose.orientation.w = quat[3]

        self.left_waypoint.pose.position.x = self.left_centroid[0]
        self.left_waypoint.pose.position.y = self.left_centroid[1]
        self.left_waypoint.pose.position.z = self.left_centroid[2]
        r = R.from_euler('x', self.left_rotation, degrees=True)
        quat = r.as_quat()
        self.left_waypoint.pose.orientation.x = quat[0]
        self.left_waypoint.pose.orientation.y = quat[1]
        self.left_waypoint.pose.orientation.z = quat[2]
        self.left_waypoint.pose.orientation.w = quat[3]

        right_text = f"(x: {np.round(self.right_centroid[0] - self.image_width/2)}, y: {np.round(self.right_centroid[1] - self.image_height/2)}, rotation: {self.right_rotation})"
        left_text = f"(x: {np.round(self.left_centroid[0] - self.image_width/2)}, y: {np.round(self.left_centroid[1] - self.image_height/2)}, rotation: {self.left_rotation})"

        annotated_image = cv.putText(annotated_image, right_text,
                                     (int(self.right_centroid[0])-100,
                                      int(self.right_centroid[1])+40),
                                     cv.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)
        
        annotated_image = cv.putText(annotated_image, left_text,
                                     (int(self.left_centroid[0])-100,
                                      int(self.left_centroid[1])+40),
                                     cv.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)

        annotated_image = cv.circle(
            annotated_image, (int(self.right_centroid[0]), int(self.right_centroid[1])), 10, (255, 255, 255), -1)

        annotated_image = cv.circle(
            annotated_image, (int(self.left_centroid[0]), int(self.left_centroid[1])), 10, (255, 255, 255), -1)

        cv_image = self.bridge.cv2_to_imgmsg(
            annotated_image, encoding="rgb8")

        return cv_image, left_gesture, right_gesture

    def process_color_image(self):
        """Process the color image to find the 3D location of the hand's pose."""
        try:
            mp_image = mp.Image(
                image_format=mp.ImageFormat.SRGB, data=self.color_image)

            detection_result = self.mps.landmarker.recognize(mp_image)
            annotated_image = self.mps.draw_landmarks_on_image(
                rgb_image=self.color_image, detection_result=detection_result)

            return annotated_image, detection_result

        except CvBridgeError:
            self.get_logger().error(CvBridgeError)

    def timer_callback(self):
        """Publish the annotated image and the waypoint for the arm"""
        if self.color_image is not None :
            annotated_image, detection_result= self.process_color_image()
            cv_image, left_gesture, right_gesture = self.process_depth_image(
                annotated_image, detection_result)
            self.cv_image_pub.publish(cv_image)
            self.right_gesture_pub.publish(String(data=right_gesture))
            self.left_gesture_pub.publish(String(data=left_gesture))


        
        # publish the waypoint
        self.right_waypoint.header.stamp = self.get_clock().now().to_msg()
        self.right_waypoint_pub.publish(self.right_waypoint)

        self.left_waypoint.header.stamp = self.get_clock().now().to_msg()
        self.left_waypoint_pub.publish(self.left_waypoint)


def main(args=None):
    rclpy.init(args=args)

    handcv = HandCV()

    rclpy.spin(handcv)


if __name__ == '__main__':
    main()
